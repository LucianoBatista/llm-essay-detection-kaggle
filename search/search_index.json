{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM - Detect AI Generated Text","text":"<p>This is kaggle competition to classify texto into AI generated or as a human write.</p> <p>Some useful informations:</p> <ul> <li>Dataset: 10,000 essays with some written by students and some generated by large language models (LLMs).</li> <li>Objective: Determine if an essay was generated by an LLM.</li> <li>Essays respond to seven prompts, with students instructed to read source texts before writing.</li> <li>Training set: Comprises essays from two prompts, mostly student-written with a few LLM-generated examples.</li> <li>Hidden test set: Comprises essays from the remaining prompts.</li> <li>Reminder that it's a Code Competition: The test data (<code>test_essays.csv</code>) is dummy data, to be replaced with a full test set of 9,000 essays (both student-written and LLM-generated) during scoring.</li> </ul>"},{"location":"#data","title":"Data","text":"<p>For test and train <code>(test|train)_essays.csv</code>:</p> <ul> <li><code>id</code>: unique identifier for the each essay</li> <li><code>prompt_id</code>: identifies the prompt the essay was written</li> <li><code>text</code>: the essay text itself</li> <li><code>generated</code>:</li> <li>0 = written by a student</li> <li>1 = generated by an LLM</li> </ul> <p>For the file <code>train_prompts.csv</code>:</p> <ul> <li><code>prompt_id</code>: unique identifier for each prompt</li> <li><code>prompt_name</code>: the title of the prompt</li> <li><code>instructions</code>: the instructions given to students</li> <li><code>source_text</code>: the text of the article(s) the essays were written in response to, in Markdown format.</li> </ul> <p>There is others information about the <code>source_text</code>:</p> <p>Significant paragraphs are enumerated by a numeral preceding the paragraph on the same line, as in 0 Paragraph one.\\n\\n1 Paragraph two.. Essays sometimes refer to a paragraph by its numeral. Each article is preceded with its title in a heading, like # Title. When an author is indicated, their name will be given in the title after by. Not all articles have authors indicated. An article may have subheadings indicated like ## Subheading.</p>"},{"location":"#generated-text-from-llms","title":"Generated text from LLMs","text":"<p>There is a huge number of available generated text from kaggle, I'll try to use those because the original training dataset has just 3 essay generated by the AI model.</p> <p>Please check this notebook if you want to use too.</p> <p></p>"},{"location":"experiment/","title":"Experiments","text":"<p>Here you'll find what each experiment on the repository does. Some experiments will contains ETL process, others will have model training, others just validating some ideas.</p>"},{"location":"experiment/#exp-01","title":"Exp 01","text":"<p>In this process I'm creating a new dataset to use on the competition. This dataset will contain a lot of essays written by humans and AI generated as well. I'm doing this because the original dataset from the competition contains almost none ai generated essays, and also because the leaderboard (test data) is evaluated on essays that was written also by another 5 prompts not showed on the training data.</p> <p>The best part is that those prompts are comming from a well known corpus, the PERSUADE corpus.</p> <p>The data generated here will contains some columns that will help us to validate different datasets from the community, I'm putting a \"kaggle_repo\" flag, to indicate from where the data come from.</p> <p>The data looks like:</p> <pre><code>shape: (54_691, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id    \u2506 prompt_id \u2506 text                              \u2506 generated \u2506 model         \u2506 kaggle_repo \u2502\n\u2502 ---   \u2506 ---       \u2506 ---                               \u2506 ---       \u2506 ---           \u2506 ---         \u2502\n\u2502 u32   \u2506 str       \u2506 str                               \u2506 i64       \u2506 str           \u2506 i64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0     \u2506 0         \u2506 Advantages of Limiting Car Usage\u2026 \u2506 1         \u2506 gpt-3.5-turbo \u2506 1           \u2502\n\u2502 1     \u2506 0         \u2506 Advantages of Limiting Car Usage\u2026 \u2506 1         \u2506 gpt-3.5-turbo \u2506 1           \u2502\n\u2502 2     \u2506 0         \u2506 Limiting car usage has numerous \u2026 \u2506 1         \u2506 gpt-3.5-turbo \u2506 1           \u2502\n\u2502 3     \u2506 0         \u2506 The passages provided discuss th\u2026 \u2506 1         \u2506 gpt-3.5-turbo \u2506 1           \u2502\n\u2502 \u2026     \u2506 \u2026         \u2506 \u2026                                 \u2506 \u2026         \u2506 \u2026             \u2506 \u2026           \u2502\n\u2502 54687 \u2506 -1        \u2506 Working alone, students do not h\u2026 \u2506 0         \u2506 human         \u2506 9           \u2502\n\u2502 54688 \u2506 -1        \u2506 \"A problem is a chance for you t\u2026 \u2506 0         \u2506 human         \u2506 9           \u2502\n\u2502 54689 \u2506 -1        \u2506 Many people disagree with Albert\u2026 \u2506 0         \u2506 human         \u2506 9           \u2502\n\u2502 54690 \u2506 -1        \u2506 Do you think that failure is the\u2026 \u2506 0         \u2506 human         \u2506 9           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Filtering duplicates and human written text (just for now), we endup with a small dataset with 823 AI generated text.</p> <p>I'm planning for the next iteration, to use this 823 plus the data from the competition to create our classifier.</p>"},{"location":"experiment/#exp-02","title":"Exp 02","text":"<p>In this experiment I tuned a RoBERTa base model, with a classification head with the output of two classes:</p> <ul> <li>1: Generated by AI</li> <li>0: Written by a student</li> </ul> <p>The objective here was to understand how I could upload a pre-trained model to the competition, without use the internet on the submission notebook.</p> <p>Besides, with this approach I want to validate also if using a transformer encoder based model could output a good metric right away or not.</p> <p>I got a very poor result on the public leaderboard with a score of 0.596, where strategies with tfidf + traditional classifiers have beeing more helpful with a ROC AUC of 0.960.</p> <p>Some param that I used:</p> <ul> <li>epochs: 20 and 100</li> <li>base model: 768 dimensions</li> <li>pretrained tokenizer: sentece-piece</li> <li>data: from the competition + external data of AI generated (I did not used others human written text)</li> </ul>"},{"location":"experiment/#exp-03","title":"Exp 03","text":"<p>As I mentioned before, the best performances are comming from a Kaggle Grandmaster that published a very good idea of use of a tuning of the tokenizer + tfidf strategies + traditional classifiers. So I had to experiment that.</p> <p>As this start I'm not just copying others solutions, and I'm using the time to learn new strategies and the what of some techniques are working and others not.</p> <p>This run was very direct, and I basically created a simple pipeline with the tfidf vectorizer and a MultinomialNB model, and run it on a database. This time I used more data during the experiment, as you can see from <code>src/exp_03/exp_03.py</code>, look how simple was the pipe:</p> <pre><code>pipeline = Pipeline(\n    [\n        (\"tfidf\", TfidfVectorizer(max_features=4500)),\n        (\"clf\", MultinomialNB()),\n    ]\n)\n</code></pre> <p>The performance was almost 0.7, where I believe that the max feature param help the model to not overfit.</p>"},{"location":"experiment/#exp-04","title":"Exp 04","text":"<p>This is an extension of the experiment 03, where I did some tuning on a tokenizer. I used this link to understand how to tuning a tokenizer, I did not used a training from scratch, supposing that this could work better.</p> <p>Here we need to do some heavy work to load some files to public datasets in order to use the notebook without internet connection for the submission.</p> <p>I did some improviment on the public leaderboard, but did not came to the top as my score was 0.795.</p> <p>This shows that this approach really has some benefits.</p> <p>I also did some small changes on this experiment, on data, tfidf params, MultinomialNB params, but nothing result in a score higher than 0.795 for this run.</p>"},{"location":"external_sources/","title":"External Souces","text":"<p>The objective here is to describe any external public available source of essay, that can be used on the competition.</p> <p>Important note, there is a <code>persuade</code> corpus, this is public, and seems that the competition is using the 2 and 12 prompts from this corpus, to give to students to write an essay.</p> <p>More info about it:</p> <ul> <li>Here</li> <li>And Here</li> </ul>"},{"location":"external_sources/#llm-generated-essays-for-the-detect-ai-comp","title":"LLM Generated Essays for the Detect AI Comp","text":"<p>This dataset contains 700 LLM generated essays in total, where:</p> <ul> <li>500 generated by the gpt-3.5-turbo</li> <li>200 generated by the gpt-4</li> </ul> <p>Those prompt ids, seems to be from the competition.</p> <ul> <li>Dataset</li> </ul>"},{"location":"external_sources/#argugpt","title":"ArguGPT","text":"<p>ArguGPT is a balanced corpus of 4,038 argumentative essays generated by 7 GPT models in response to essay prompts from three sources:</p> <ol> <li>in-class or homework exercises</li> <li>TOEFL</li> <li>GRE writing tasks.</li> </ol> <p>Those models was not identified, the paper says the following:</p> <p>...seven models of the GPT family (GPT2-XL, variants of GPT3, and ChatGPT)</p> <p>One important observation is that the paper also evaluate human written text, but I couldn't find those texts.</p> <p>I'll link to paper, very useful information there:</p> <ul> <li>Paper</li> <li>Dataset</li> </ul>"},{"location":"external_sources/#daigt-external-dataset","title":"DAIGT | External Dataset","text":"<p>I'm using just the AI generated part of the dataset. This dataset also has a column of student written essays, but it is a bit confusing about how this is structure. This dataset was generated by using chatgpt.</p> <ul> <li>Dataset</li> </ul>"},{"location":"external_sources/#daigt-data-llama-70b-and-falcon-180b","title":"DAIGT Data | Llama 70b and Falcon 180b","text":"<p>8K of generated essays each from Llama 70b and Falcon 180b.</p> <p>There is different csvs, and the prompts were from the PERSUADE database and from GPT-4.</p> <ul> <li>Dataset</li> </ul>"},{"location":"external_sources/#llm-generated-essay-using-palm-from-google-gen-ai","title":"LLM-generated essay using PaLM from Google Gen-AI","text":"<p>This database is simple, but I think that is interesting to use, because is using those <code>prompt_id</code>s from the original training dataset of the competition.</p> <p>One downside is that the prompt id can be limited to training dataset. As the competition says that we have 7 prompts in total, add some others prompts to the training data can be interesting.</p> <ul> <li>Dataset</li> </ul>"},{"location":"external_sources/#hello-claude-1000-essays-from-7-persuade-prompts","title":"Hello, Claude! 1000 Essays from 7 Persuade prompts","text":"<p>Basically as used the <code>claude-instant-1</code> model, using all prompts from PERSUADE dataset. We have essays from those two prompts on the training data, but also about the other prompts.</p> <ul> <li>Dataset</li> </ul>"},{"location":"external_sources/#persuade-corpus-20","title":"Persuade Corpus 2.0","text":"<p>This dataset has aprox. 26k essays, from all the 7 Prompts that probably exist on the competition. And all data was written by a human. I thinks this could help to the model abstract better to the leaderboard score.</p> <ul> <li>Dataset</li> </ul>"},{"location":"external_sources/#daigt-proper-train-dataset","title":"DAIGT Proper Train Dataset","text":"<p>This dataset is not very much explained about the difference between all those versions of csvs. But I'm using just two csvs that have data generated by Mistral.</p> <p>I opt for that because Mistral was not very used on above showed datasets.</p> <ul> <li>Dataset</li> </ul>"},{"location":"external_sources/#feedback-prize-3","title":"Feedback Prize 3","text":"<p>As we are evaluating if the essay was written by a person or not, may have more data outside those prompts could help. This dataset is from the Feedback Prize 3 Kaggle Competition, offered by the same lab (The Learning Agency Lab), and contains just essays written by students (about 3911 essays).</p> <ul> <li>Dataset</li> </ul>"},{"location":"external_sources/#important","title":"Important","text":"<p>Search if there is human written essays using some of the 7 PERSUADE prompts!</p>"}]}